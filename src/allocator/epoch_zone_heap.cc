/*
 *  (c) Copyright 2016-2021,2023 Hewlett Packard Enterprise Development
 *  Company LP.
 *
 *  This software is available to you under a choice of one of two
 *  licenses. You may choose to be licensed under the terms of the GNU Lesser
 *  General Public License Version 3, or (at your option) later with exceptions
 *  included below, or under the terms of the  MIT license (Expat) available in
 *  COPYING file in the source tree.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU Lesser General Public License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *
 *  As an exception, the copyright holders of this Library grant you permission
 *  to (i) compile an Application with the Library, and (ii) distribute the
 *  Application containing code generated by the Library and added to the
 *  Application during this compilation process under terms of your choice,
 *  provided you also meet the terms and conditions of the Application license.
 *
 */

#include <stddef.h>
#include <stdint.h>

#include <assert.h>
#include <string>

#include "nvmm/error_code.h"
#include "nvmm/fam.h"
#include "nvmm/global_ptr.h"
#include "nvmm/heap.h"
#include "nvmm/log.h"
#include "nvmm/memory_manager.h"
#include "nvmm/shelf_id.h"

#include "shelf_mgmt/pool.h"

#include "shelf_usage/shelf_region.h"
#include "shelf_usage/zone_shelf_heap.h"

#include "nvmm/epoch_manager.h"

#include "allocator/epoch_zone_heap.h"

#include "common/common.h"
#include "common/config.h"

namespace nvmm {

#define CHECK_IS_CLOSED()                                                      \
    if (IsOpen() == true) {                                                    \
        LOG(error) << "Heap is already open";                                  \
        return HEAP_IS_OPEN;                                                   \
    }

#define CHECK_IS_OPEN()                                                        \
    if (IsOpen() != true) {                                                    \
        LOG(error) << "Heap is not open";                                      \
        return HEAP_NOT_OPEN;                                                  \
    }

#define ASSERT_IS_CLOSED() assert(IsOpen() != true);
#define ASSERT_IS_OPEN() assert(IsOpen() == true);

// If LFS_BOOK_SIZE is not passed, set it to 8GB.
#ifndef LFS_BOOK_SIZE
#define LFS_BOOK_SIZE (8 * 1024 * 1024 * 1024UL)
#endif

inline uint64_t power_of_two(uint64_t n) {
    uint64_t power = 0;
    if (n == 0) {
        return 0;
    }
    while (n != 1) {
        if (n % 2 != 0) {
            return 0;
        }
        n = n >> 1;
        power = power + 1;
    }
    return power;
}

inline bool is_power_of_two(uint64_t n) { return ((n != 0) && !(n & (n - 1))); }

inline uint64_t next_power_of_two(uint64_t n) {
    if (is_power_of_two(n)) {
        return n;
    } else {
        n |= n >> 1;
        n |= n >> 2;
        n |= n >> 4;
        n |= n >> 8;
        n |= n >> 16;
        n |= n >> 32;
        return n + 1;
    }
}

#define MAX(a, b) (((a) > (b)) ? (a) : (b))

//
// There is a bug in current LFS code, which does not mmap from right offset.
// Bug: mmap from any offset incorrectly mmaps from boundaries of Book size
// (8GB default).
//
// So we create header's in case of LFS as the multiple of 8GB (Book size).
// Header extention in case of a Resize also will be multiple of 8GB(Book size).
//
inline size_t header_size_round_up() {
#ifdef LFSWORKAROUND
    return round_up(LFS_BOOK_SIZE, getpagesize());
#else
    return MAX(config.PageSize, getpagesize());
#endif
}

EpochZoneHeap::EpochZoneHeap(PoolId pool_id)
    : gh_{NULL}, pool_id_{pool_id}, pool_{pool_id}, rmb_addr_{NULL},
      rmb_size_{0}, rmb_{NULL}, region_{NULL},
      mapped_addr_{NULL}, header_{NULL}, is_open_{false}, is_invalid_{false},
      no_bgthread_{false}, fast_alloc_{0}, cleaner_start_{false},
      cleaner_stop_{false}, cleaner_running_{false} {}

EpochZoneHeap::~EpochZoneHeap() {
    if (IsOpen() == true) {
        (void)Close();
    }
}

// Creates a heap with size next power of two of the passed size
ErrorCode EpochZoneHeap::Create(size_t shelf_size, size_t min_alloc_size,
                                uint64_t flags, mode_t mode) {
    TRACE();

    //
    // Create creates one header and one data shelf.
    // So we will be using 0 only as index for data and header data
    // structures.
    //
    int shelf_num = 0;
    CHECK_IS_CLOSED();
    if (pool_.Exist() == true) {
        return POOL_FOUND;
    } else {
        ErrorCode ret = NO_ERROR;

        // Round off the shelf size to next power of 2
        shelf_size = next_power_of_two(shelf_size);

        // If shelf_size is greater than MAX_ZONE_SIZE return error
        if (shelf_size > MAX_ZONE_SIZE) {
          return HEAP_CREATE_FAILED;
        }

        // create an empty pool
        ret = pool_.Create(shelf_size, mode);
        if (ret != NO_ERROR) {
            return HEAP_CREATE_FAILED;
        }

        // add two shelves, one for zone, one for the headers
        ret = pool_.Open(false);
        if (ret != NO_ERROR) {
            return HEAP_CREATE_FAILED;
        }
        ShelfIndex shelf_idx;

        shelf_idx = kHeaderIdx;
        header_size_ =
            get_header_size_from_size(shelf_size, min_alloc_size, shelf_idx);
        ret = pool_.AddShelf(shelf_idx,
                             [this](ShelfFile *shelf, size_t shelf_size) {
                                 ShelfRegion shelf_region(shelf->GetPath());
                                 // TODO: this should be
                                 // shelf_size/min_obj_size*zone_entry_size
                                 return shelf_region.Create(header_size_);
                             },
                             false, mode);
        if (ret != NO_ERROR) {
            (void)pool_.Close(false);
            return HEAP_CREATE_FAILED;
        }

        std::string path;

        // get the header shelf
        ret = pool_.GetShelfPath(kHeaderIdx, path);
        if (ret != NO_ERROR) {
            return HEAP_CREATE_FAILED;
        }

        // open region
        region_ = new ShelfRegion(path);
        assert(region_ != NULL);
        ret = region_->Open(O_RDWR);
        if (ret != NO_ERROR) {
            LOG(error) << "Zone: region open failed " << (uint64_t)pool_id_;
            return HEAP_CREATE_FAILED;
        }

        // map region
        // size_t header_size = region_->Size();
        ret = region_->Map(NULL, header_size_, PROT_READ | PROT_WRITE,
                           MAP_SHARED, 0, (void **)&mapped_addr_[shelf_num]);
        if (ret != NO_ERROR) {
            LOG(error) << "Zone: region map failed " << (uint64_t)pool_id_;
            return HEAP_CREATE_FAILED;
        }

        // map the global header
        gh_ = (GlobalHeader *)mapped_addr_[shelf_num];

        // reserve the 5 global lists for delayed free
        uint64_t reserved =
            round_up(kListCnt * sizeof(ZoneEntryStack), kCacheLineSize) +
            round_up(sizeof(struct GlobalHeader), kCacheLineSize);

        fam_memset_persist(mapped_addr_[shelf_num], 0, reserved);

        // use this region to help create the zone
        header_[shelf_num] =
            (void *)((char *)mapped_addr_[shelf_num] + reserved);

        shelf_idx = kZoneIdx;
        min_obj_size_ = min_alloc_size;
        if (flags & NVMM_FAST_ALLOC) {
            fast_alloc_ = 1;
        }
        header_size_ = header_size_ - reserved;
        ret = pool_.AddShelf(shelf_idx,
                             [this](ShelfFile *shelf, size_t shelf_size) {
                                 ShelfHeap shelf_heap(shelf->GetPath());
                                 return shelf_heap.Create(
                                     shelf_size, header_[0], header_size_,
                                     min_obj_size_, fast_alloc_);
                             },
                             false, mode);
        if (ret != NO_ERROR) {
            // unmap and close the region
            ret = region_->Unmap(mapped_addr_[shelf_num],
                                 header_size_ + reserved);
            if (ret != NO_ERROR) {
                return HEAP_CREATE_FAILED;
            }
            ret = region_->Close();
            if (ret != NO_ERROR) {
                return HEAP_CREATE_FAILED;
            }
            delete region_;
            (void)pool_.Close(false);
            return HEAP_CREATE_FAILED;
        }
        //
        // TODO: Do we need atomic operation here? May not, As create
        // is already protected by locks.
        //
        fam_atomic_u64_write(&gh_->sz[0].headersize, header_size_ + reserved);
        fam_atomic_u64_write(&gh_->sz[0].headeroffset, 0);
        fam_atomic_u64_write(&gh_->sz[0].shelfsize, shelf_size);
        fam_atomic_u64_write(&gh_->total_shelfs, 1);
        fam_atomic_u64_write(&gh_->fast_alloc, fast_alloc_);

        // unmap and close the region
        ret = region_->Unmap(mapped_addr_[shelf_num], header_size_ + reserved);
        if (ret != NO_ERROR) {
            return HEAP_CREATE_FAILED;
        }
        ret = region_->Close();
        if (ret != NO_ERROR) {
            return HEAP_CREATE_FAILED;
        }
        delete region_;

        ret = pool_.Close(false);
        if (ret != NO_ERROR) {
            return HEAP_CREATE_FAILED;
        }

        return ret;
    }
}

//
// 1. The input to the resize is new total size
//    If new size less than existing size, we do not need to do anything
//    - return NO_ERROR.
//
// 2. The output is new shelf index which is optional. If the new_shelf_idx
// pointer is not NULL, 	  the new shelf index is assigned to this pointer. By
// default the output pointer is set to NULL.
//
// 3. Zone has a restriction that every shelf should be a power of 2.
//    So convert the size to next power of 2.
//
ErrorCode EpochZoneHeap::Resize(size_t size, ShelfIndex *new_shelf_idx) {
    TRACE();

    if (size > MAX_ZONE_SIZE) {
      return HEAP_RESIZE_FAILED;
    }

    // TODO: Currently resize can be performed only on open heap
    if (IsOpen() != true) {
        LOG(error) << "Heap is not open";
        return HEAP_NOT_OPEN;
    }
    LOG(trace) << "Resize Heap, size: " << size;
    ErrorCode ret = NO_ERROR;

    // Set the resize in progress bit
    uint64_t old_value = 0;
    uint64_t new_value = OP_RESIZE;
    old_value =
        fam_atomic_u64_compare_and_store(&gh_->op_in_progress, 0, new_value);
    if (old_value != 0) {
        return HEAP_BUSY;
    }

    mode_t perm;
    // Read permission, So that we use latest permission
    ret = region_->GetPermission(&perm);
    if (ret != NO_ERROR) {
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return HEAP_RESIZE_FAILED;
    }

    // Get current total data shelfs.
    int shelf_num = get_total_data_shelfs();
    // We will be able to create kMaxShelfCount - 1 shelfs as one shelf is
    // needed for header.
    if (shelf_num >= (Pool::kMaxShelfCount - 1)) {
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return HEAP_RESIZE_FAILED;
    }
    //
    // We need to get the new header size,
    // 1. Current total header size, current_header_size
    // 2. Required header size, new_header_size
    // total_header_size = current_header_size + new_header_size
    //
    // Get current header size and total shelf size.
    size_t current_header_size = get_total_header_size();
    size_t current_total_size = get_total_size();

    // If we are already at the required size, Do not resize
    if (size <= current_total_size) {
        LOG(trace) << "Already at the specified size, no need to resize, "
                      "current size: "
                   << size << " Requested size: " << current_total_size;
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return NO_ERROR;
    }
    // Convert the new shelf size to power of 2
    shelf_size_for_create_ = next_power_of_two(size - current_total_size);

    // Get the required header size for new shelf
    size_t new_header_size = get_header_size_from_size(
        shelf_size_for_create_, min_obj_size_, (ShelfIndex)shelf_num);
    // Add this to total header size
    size_t total_header_size = new_header_size + current_header_size;

    // Check if a previous resize has already resized the header more than
    // current required value
    if (region_->Size() >= total_header_size) {
        LOG(trace) << "Header already resized, skipping resize header";
    } else {
        // Truncate the header
        ret = region_->Resize(total_header_size);
        if (ret != NO_ERROR) {
            fam_atomic_u64_write(&gh_->op_in_progress, 0);
            return ret;
        }
    }
    // map the header
    ret =
        region_->Map(NULL, new_header_size, PROT_READ | PROT_WRITE, MAP_SHARED,
                     current_header_size, (void **)&mapped_addr_[shelf_num]);
    if (ret != NO_ERROR) {
        LOG(error) << "Zone: region map failed " << (uint64_t)pool_id_;
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return HEAP_RESIZE_FAILED;
    }

    // Set header_ , global_list_
    uint64_t reserved =
        round_up(kListCnt * sizeof(ZoneEntryStack), kCacheLineSize);
    fam_memset_persist(mapped_addr_[shelf_num], 0, reserved);

    header_[shelf_num] = (void *)((char *)mapped_addr_[shelf_num] + reserved);
    header_size_ = total_header_size - current_header_size;

    ShelfIndex shelf_idx = (ShelfIndex)(shelf_num + 1);

    // Set output to new shelf index, if pointer is not NULL.
    if (new_shelf_idx)
        *new_shelf_idx = shelf_idx - 1;
    shelf_id_for_create_ = (int)shelf_idx;

    // Check if previous unfinished resize has a shelf created.
    // If yes remove the shelf
    if (pool_.CheckShelf(shelf_idx) == true) {
        LOG(trace)
            << "Zone: Removing shelf created by previous unfinished resize "
            << (uint64_t)pool_id_ << " " << (int)shelf_idx;
        ret = pool_.RemoveShelf(shelf_idx);
        if (ret != NO_ERROR) {
            fam_atomic_u64_write(&gh_->op_in_progress, 0);
            region_->Unmap(mapped_addr_[shelf_num], new_header_size);
            return HEAP_RESIZE_FAILED;
        }
    }
    // Create new shelf
    ret = pool_.AddShelf(shelf_idx,
                         [this](ShelfFile *shelf, size_t shelf_size) {
                             ShelfHeap shelf_heap(shelf->GetPath());
                             return shelf_heap.Create(
                                 shelf_size_for_create_,
                                 header_[shelf_id_for_create_ - 1],
                                 header_size_, min_obj_size_, fast_alloc_);
                         },
                         false, perm);
    if (ret != NO_ERROR) {
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        region_->Unmap(mapped_addr_[shelf_num], new_header_size);
        return ret;
    }
    // Update the gh_ region
    fam_atomic_u64_write(&gh_->sz[shelf_num].headersize, new_header_size);
    fam_atomic_u64_write(&gh_->sz[shelf_num].headeroffset, current_header_size);
    fam_atomic_u64_write(&gh_->sz[shelf_num].shelfsize, shelf_size_for_create_);
    fam_atomic_64_fetch_add(&(int64_t &)gh_->total_shelfs, 1);
    fam_atomic_u64_write(&gh_->op_in_progress, 0);

    // Open the newly added shelfs
    ret = OpenNewShelfs();
    return ret;
}

//
// SetPermission : PreConditions : Heap needs to be Opened.
// @param mode : New permission to be set.
//
// Code Flow :
// 1. Check if Heap Open, If not Open, return Error.
// 2. Check if any other metadata operation in progress.
//    If yes return Operation in Progress.
// 3. Set PERMISSION_CHANGE_IN_PROGRESS bit.
// 4. Set permission of metadata shelf.
// 5. Set permission of all the data shelfs.
// 6. Set permission of header shelf.
// 7. Reset PERMISSION_CHANGE_IN_PROGRESS bit.
//
ErrorCode EpochZoneHeap::SetPermission(mode_t mode) {
    TRACE();

    if (IsOpen() != true) {
        LOG(error) << "Heap is not open";
        return HEAP_NOT_OPEN;
    }
    LOG(trace) << "ChangePermission, mode: " << mode;
    ErrorCode ret = NO_ERROR;

    // Set the change permission in progress bit
    uint64_t old_value = 0;
    uint64_t new_value = OP_CHANGE_PERM;
    old_value =
        fam_atomic_u64_compare_and_store(&gh_->op_in_progress, 0, new_value);
    if (old_value != 0) {
        return HEAP_BUSY;
    }

    // Check current permission and proposed permission, if it is same
    // Do not set the permission
    mode_t perm;
    ret = region_->GetPermission(&perm);
    if (ret != NO_ERROR) {
        LOG(fatal) << "SetPermission: Reading current permission failed: "
                   << ret;
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return HEAP_SET_PERMISSION_FAILED;
    }
    if ((mode & PERM_MASK) == perm) {
        LOG(trace) << "Already set to requested permission, Returning success";
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return NO_ERROR;
    }

    // Set permission of pool metadata shelf
    ret = pool_.SetPermission(mode);
    if (ret != NO_ERROR) {
        LOG(fatal) << "SetPermission: MetadataShelf set permission failed:"
                   << ret;
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return HEAP_SET_PERMISSION_FAILED;
    }

    // Open the newly added shelfs
    ret = OpenNewShelfs();
    // Set permission of all shelfs in a loop
    int total_shelfs = get_total_data_shelfs();
    for (int i = 0; i < total_shelfs; i++) {
        ret = rmb_[i]->SetPermission(mode);
        if (ret != NO_ERROR) {
            LOG(fatal) << "SetPermission: set permission failed for shelf"
                       << i + 1 << " with " << ret;
            fam_atomic_u64_write(&gh_->op_in_progress, 0);
            return HEAP_SET_PERMISSION_FAILED;
        }
    }

    //
    // Finally, Set permission of header. GetPermission reads the permission of
    // header, and returns it to user. So the permission of file header is
    // always right.
    //
    ret = region_->SetPermission(mode);
    if (ret != NO_ERROR) {
        LOG(fatal) << "SetPermission: Header change permission failed:" << ret;
        fam_atomic_u64_write(&gh_->op_in_progress, 0);
        return HEAP_SET_PERMISSION_FAILED;
    }
    // Reset in progress bit
    fam_atomic_u64_write(&gh_->op_in_progress, 0);
    return NO_ERROR;
}

ErrorCode EpochZoneHeap::GetPermission(mode_t *mode) {
    TRACE();
    if (IsOpen() == false)
        return HEAP_GET_PERMISSION_FAILED;
    if (region_->GetPermission(mode) != NO_ERROR) {
        return HEAP_GET_PERMISSION_FAILED;
    }
    return NO_ERROR;
}

ErrorCode EpochZoneHeap::Destroy() {
    TRACE();
    CHECK_IS_CLOSED();
    if (pool_.Exist() == false) {
        return POOL_NOT_FOUND;
    } else {
        ErrorCode ret = NO_ERROR;

        // remove both shelves
        ret = pool_.Open(false);
        if (ret != NO_ERROR) {
            return HEAP_DESTROY_FAILED;
        }
        {
            std::string path;
            // get the header shelf
            ret = pool_.GetShelfPath(kHeaderIdx, path);
            if (ret != NO_ERROR) {
                (void)pool_.Close(false);
                return HEAP_DESTROY_FAILED;
            }
            // open region
            region_ = new ShelfRegion(path);
            assert(region_ != NULL);
            ret = region_->Open(O_RDWR);
            if (ret != NO_ERROR) {
                LOG(error) << "Zone: region open failed " << (uint64_t)pool_id_;
                (void)pool_.Close(false);
                return HEAP_DESTROY_FAILED;
            }

            // Open gh_
            ret = region_->Map(
                NULL, round_up(sizeof(struct GlobalHeader), kCacheLineSize),
                PROT_READ | PROT_WRITE, MAP_SHARED, 0, (void **)&gh_);
            if (ret != NO_ERROR) {
                LOG(error) << "Zone: region map failed " << (uint64_t)pool_id_;
                ret = region_->Close();
                if (ret != NO_ERROR) {
                    return HEAP_DESTROY_FAILED;
                }
                delete region_;
                (void)pool_.Close(false);
                return HEAP_DESTROY_FAILED;
            }
            fam_atomic_u64_write(&gh_->destroy_in_progress, 1);
        }
        ret = pool_.Recover();
        if (ret != NO_ERROR) {
            LOG(fatal) << "Destroy: Found inconsistency in Heap "
                       << (uint64_t)pool_id_;
        }
        int total_data_shelfs = get_total_data_shelfs();
        if (total_data_shelfs == -1) {
            LOG(fatal) << "Destroy: Could not find total data shelfs "
                       << (uint64_t)pool_id_;
            return HEAP_DESTROY_FAILED;
        }

        // Unmap and close the region
        (void)region_->Unmap(
              gh_, round_up(sizeof(struct GlobalHeader), kCacheLineSize));
        ret = region_->Close();
        if (ret != NO_ERROR) {
            return HEAP_DESTROY_FAILED;
        }
        delete region_;   

        std::string path;
        for (int shelf_num = 0; shelf_num < total_data_shelfs; shelf_num++) {

            ShelfIndex shelf_idx = (ShelfIndex)(shelf_num + 1);
            ret = pool_.GetShelfPath(shelf_idx, path);
            assert(ret == NO_ERROR);
            ShelfHeap shelf_heap(path, ShelfId(pool_id_, shelf_idx));
            shelf_heap.Destroy();

            ret = pool_.RemoveShelf(shelf_idx);
            if (ret != NO_ERROR) {
                (void)pool_.Close(false);
                return HEAP_DESTROY_FAILED;
            }
        }
        ret = pool_.GetShelfPath(kHeaderIdx, path);
        assert(ret == NO_ERROR);
        ShelfRegion shelf_region(path);
        shelf_region.Destroy();

        ret = pool_.RemoveShelf(kHeaderIdx);
        if (ret != NO_ERROR) {
            (void)pool_.Close(false);
            return HEAP_DESTROY_FAILED;
        }

        ret = pool_.Close(false);
        if (ret != NO_ERROR) {
            return HEAP_DESTROY_FAILED;
        }

        // destroy the pool
        ret = pool_.Destroy();
        if (ret != NO_ERROR) {
            return HEAP_DESTROY_FAILED;
        }
        return ret;
    }
}

bool EpochZoneHeap::Exist() { return pool_.Exist(); }

ErrorCode EpochZoneHeap::Open(int flags) {
    TRACE();
    LOG(trace) << "Open Heap " << (uint64_t)pool_id_;
    CHECK_IS_CLOSED();
    ErrorCode ret = NO_ERROR;

    // open the pool
    ret = pool_.Open(false);
    if (ret != NO_ERROR) {
        return HEAP_OPEN_FAILED;
    }

    // // perform recovery
    // ret = pool_.Recover();
    // if(ret != NO_ERROR)
    // {
    //     LOG(fatal) << "Open: Found inconsistency in Region " <<
    // (uint64_t)pool_id_;
    // }

    std::string path;

    // get the header shelf
    ret = pool_.GetShelfPath(kHeaderIdx, path);
    if (ret != NO_ERROR) {
        (void)pool_.Close(false);
        return HEAP_OPEN_FAILED;
    }

    // open region
    region_ = new ShelfRegion(path);
    assert(region_ != NULL);
    ret = region_->Open(O_RDWR);
    if (ret != NO_ERROR) {
        LOG(error) << "Zone: region open failed " << (uint64_t)pool_id_;
        (void)pool_.Close(false);
        return HEAP_OPEN_FAILED;
    }

    // Open gh_
    ret = region_->Map(NULL,
                       round_up(sizeof(struct GlobalHeader), kCacheLineSize),
                       PROT_READ | PROT_WRITE, MAP_SHARED, 0, (void **)&gh_);
    if (ret != NO_ERROR) {
        LOG(error) << "Zone: region map failed " << (uint64_t)pool_id_;
        ret = region_->Close();
        if (ret != NO_ERROR) {
            return HEAP_OPEN_FAILED;
        }
        delete region_;
        (void)pool_.Close(false);
        return HEAP_OPEN_FAILED;
    }

    fast_alloc_ = fam_atomic_u64_read(&gh_->fast_alloc);

    int total_data_shelfs = get_total_data_shelfs();

    // Loop to open all the shelfs
    int shelf_num = 0;
    do {
        ret = OpenShelf(shelf_num);
        if (ret != NO_ERROR) {
            LOG(error) << "ZoneHeap: OpenShelf  " << shelf_num
                       << "failed for : " << (uint64_t)pool_id_;
            // Close all the OpenShelfs() and return error.
            // The current shelf will be closed by OpenShelf code,
            // Explicitely close other shelfs
            while (shelf_num > 0) {
                shelf_num--;
                ret = CloseShelf(shelf_num);
                if (ret != NO_ERROR) {
                    LOG(trace)
                        << "ZoneHeap: Open Error Handling, CloseShelf  "
                        << shelf_num << "failed for : " << (uint64_t)pool_id_;
                    // Do not return error here, we will attempt to close
                    // everything.
                }
            }

            (void)region_->Unmap(
                gh_, round_up(sizeof(struct GlobalHeader), kCacheLineSize));
            if (ret != NO_ERROR) {
                LOG(trace)
                    << "ZoneHeap: Open Error Handling, gh_ unmap failed for : "
                    << (uint64_t)pool_id_;
                // Do not return error here, we will attempt to close
                // everything.
            }
            ret = region_->Close();
            if (ret != NO_ERROR) {
                LOG(trace) << "ZoneHeap: Open Error Handling, region close "
                              "failed for : "
                           << (uint64_t)pool_id_;
                // Do not return error here, we will attempt to close
                // everything.
            }
            delete region_;
            (void)pool_.Close(false);
            return HEAP_OPEN_FAILED;
        }
        shelf_num++;
    } while (shelf_num < total_data_shelfs);

    min_obj_size_ = rmb_[0]->MinAllocSize();
    is_open_ = true;

    // If No Background thread flag specified, return without spawning threads
    if(flags & NVMM_NO_BG_THREAD){ 
       no_bgthread_ = true;
       return NO_ERROR;
    }

    // start the cleaner thread
    int rc = StartWorker();
    if (rc != 0) {
        // Close all the OpenShelfs() and return error.
        do {
            shelf_num--;
            ret = CloseShelf(shelf_num);
            if (ret != NO_ERROR) {
                return HEAP_OPEN_FAILED;
            }
        } while (shelf_num > 0);
        return HEAP_OPEN_FAILED;
    }

    // wait for the cleaner to be running
    std::unique_lock<std::mutex> mutex(cleaner_mutex_);
    running_cv_.wait(mutex, [this] { return cleaner_running_ == true; });
    no_bgthread_ = false;
    return ret;
}

ErrorCode EpochZoneHeap::Close() {
    TRACE();
    LOG(trace) << "Close Heap " << (uint64_t)pool_id_;
    CHECK_IS_OPEN();
    ErrorCode ret = NO_ERROR;

    if (!is_invalid_ && (no_bgthread_ == false)) {
        // stop the cleaner thread
        int rc = StopWorker();
        if (rc != 0) {
            return HEAP_CLOSE_FAILED;
        }
    }
    // close the rmb
    for (int shelf_num = (int)(total_mapped_shelfs_ - 1); shelf_num >= 0;
         shelf_num--) {
        ret = CloseShelf(shelf_num);
        if (ret != NO_ERROR) {
            return HEAP_CLOSE_FAILED;
        }
    }
    // Unmap the gh_
    ret = region_->Unmap(gh_,
                         round_up(sizeof(struct GlobalHeader), kCacheLineSize));
    if (ret != NO_ERROR) {
        return HEAP_CLOSE_FAILED;
    }
    gh_ = NULL;
    ret = region_->Close();
    if (ret != NO_ERROR) {
        return HEAP_CLOSE_FAILED;
    }
    delete region_;
    region_ = NULL;

    // // perform recovery
    // ret = pool_.Recover();
    // if(ret != NO_ERROR)
    // {
    //     LOG(fatal) << "Close: Found inconsistency in Heap " <<
    // (uint64_t)pool_id_;
    // }

    // close the pool
    ret = pool_.Close(false);
    if (ret != NO_ERROR) {
        return HEAP_CLOSE_FAILED;
    }

    is_open_ = false;

    assert(ret == NO_ERROR);
    return ret;
}

size_t EpochZoneHeap::Size() {
    ASSERT_IS_OPEN();
    OpenNewShelfs();
    size_t total_size = 0;
    for (int shelf_num = 0; shelf_num < total_mapped_shelfs_; shelf_num++)
        total_size += rmb_[shelf_num]->Size();
    return total_size;
}

GlobalPtr EpochZoneHeap::Alloc(size_t size) {
    ASSERT_IS_OPEN();
    GlobalPtr ptr;
    Offset offset = 0;
    int shelf_num = -1;
    // int total_shelf = get_total_data_shelfs();
    int total_shelf = total_mapped_shelfs_;
    do {
        shelf_num = (shelf_num + 1);

        if (shelf_num == total_mapped_shelfs_) {
            total_shelf = get_total_data_shelfs();
            if (total_shelf > total_mapped_shelfs_) {
                OpenNewShelfs();
            } else
                break;
        }
        offset = rmb_[shelf_num]->Alloc(size);
    } while (rmb_[shelf_num]->IsValidOffset(offset) == false &&
             shelf_num < total_mapped_shelfs_);
    if (offset == 0)
        return 0;
    else
        return GlobalPtr(ShelfId(pool_id_, (ShelfIndex)(shelf_num + 1)),
                         offset);
}

// The offset returned by AllocOffset has Offset + ((shelf_idx-1) <<
// kOffsetShift)
Offset EpochZoneHeap::AllocOffset(size_t size) {
    GlobalPtr global_ptr = Alloc(size);

    Offset offset = global_ptr.GetOffset();
    if (offset == 0)
        return 0;
    ShelfId shelf_id = global_ptr.GetShelfId();
    ShelfIndex shelf_idx = shelf_id.GetShelfIndex();
    // Left shift shelf_idx by kOffsetShift.
    // We cant use kOffsetShift directly as it is a private member of
    // global_ptr. So we create a GlobalPtr with poolId 0 and shelfIndex as
    // shelf_idx and return uint64_t.
    return GlobalPtr(ShelfId(0, (ShelfIndex)((int)shelf_idx - 1)), offset)
        .ToUINT64();
}

void EpochZoneHeap::Free(GlobalPtr global_ptr) {
    ASSERT_IS_OPEN();
    Offset offset = global_ptr.GetOffset();
    ShelfId shelf_id = global_ptr.GetShelfId();
    ShelfIndex shelf_idx = shelf_id.GetShelfIndex();

    // The shelf is not yet open
    if (shelf_idx > total_mapped_shelfs_) {
        ErrorCode ret = OpenNewShelfs();
        // Free does not return any value, So we will need to throw exception
        if (ret != NO_ERROR) {
            LOG(trace) << "mapping new shelf failed: " << ret;
            return;
        }
    }

    rmb_[shelf_idx - 1]->Free(offset);
}

//
// Offset has the (shelfindex - 1) also in it.
// i.e offset = ((shelfIndex - 1) << 40) + offset
//
void EpochZoneHeap::Free(Offset offset) {
    ASSERT_IS_OPEN();

    // Get the shelfIndex from shelfIndex + offset
    int shelf_num = get_shelfnum_from_shelfIndexoffset(offset);

    // Get only the offset from shelfIndex + offset
    offset = get_offset_from_shelfIndexoffset(offset);

    if (shelf_num >= total_mapped_shelfs_) {
        ErrorCode ret = OpenNewShelfs();
        // TODO: Free does not return any value, So we will need to throw
        // exception
        if (ret != NO_ERROR) {
            LOG(trace) << "mapping new shelf failed: " << ret;
            return;
        }
    }
    rmb_[shelf_num]->Free(offset);
}

GlobalPtr EpochZoneHeap::Alloc(EpochOp &op, size_t size) {
    ASSERT_IS_OPEN();
    (void)op; // we don't use epoch to do allocation, but this allocation must
              // be in an EpochOp
    return Alloc(size);
}

void EpochZoneHeap::Free(EpochOp &op, Offset offset) {
    ASSERT_IS_OPEN();
    // Get the shelfIndex from shelfIndex + offset
    int shelf_num = get_shelfnum_from_shelfIndexoffset(offset);
    // Get only the offset from shelfIndex + offset
    offset = get_offset_from_shelfIndexoffset(offset);

    if ((shelf_num + 1) > total_mapped_shelfs_) {
        ErrorCode ret = OpenNewShelfs();
        // TODO: Free does not return any value, So we will need to throw
        // exception
        if (ret != NO_ERROR) {
            LOG(trace) << "mapping new shelf failed: " << ret;
            return;
        }
    }

    if (rmb_[shelf_num]->IsValidOffset(offset) == false) {
        return;
    }

    {
        EpochCounter e = op.reported_epoch();
        global_list_[shelf_num][(e + 3) % kListCnt].push(
            bitmap_start_[shelf_num], offset / min_obj_size_);
    }
}

void EpochZoneHeap::Free(EpochOp &op, GlobalPtr global_ptr) {
    ASSERT_IS_OPEN();
    Offset offset = global_ptr.GetOffset();
    ShelfId shelf_id = global_ptr.GetShelfId();
    ShelfIndex shelf_idx = shelf_id.GetShelfIndex();

    if (shelf_idx > total_mapped_shelfs_) {
        ErrorCode ret = OpenNewShelfs();
        // TODO: Free does not return any value, So we will need to throw
        // exception
        if (ret != NO_ERROR) {
            LOG(trace) << "mapping new shelf failed: " << ret;
            return;
        }
    }
    if (rmb_[shelf_idx - 1]->IsValidOffset(offset) == false)
        return;

    {
        EpochCounter e = op.reported_epoch();
        LOG(trace) << "delay freeing block [" << offset << "] at epoch "
                   << e + 3;
        global_list_[shelf_idx - 1][(e + 3) % kListCnt].push(
            bitmap_start_[shelf_idx - 1], offset / min_obj_size_);
    }
}

ErrorCode EpochZoneHeap::OpenShelf(int shelf_num) {
    std::string path;

    uint64_t headersize = fam_atomic_u64_read(&gh_->sz[shelf_num].headersize);
    uint64_t headeroffset =
        fam_atomic_u64_read(&gh_->sz[shelf_num].headeroffset);
    uint64_t shelfsize = fam_atomic_u64_read(&gh_->sz[shelf_num].shelfsize);
    // map the header for shelf 'shelf_num' if not mapped
    ErrorCode ret;
    if (mapped_addr_[shelf_num] == NULL) {
      ret = region_->Map(NULL, headersize, PROT_READ | PROT_WRITE, MAP_SHARED,
                         headeroffset, (void **)&mapped_addr_[shelf_num]);
      if (ret != NO_ERROR) {
        LOG(error) << "Zone: region map failed " << (uint64_t)pool_id_;
        return HEAP_OPEN_FAILED;
      }
    }
    size_t gh_size = 0;
    if (shelf_num == 0)
        gh_size = round_up(sizeof(struct GlobalHeader), kCacheLineSize);

    // get the global freelists
    global_list_[shelf_num] =
        (ZoneEntryStack *)((char *)mapped_addr_[shelf_num] + gh_size);
    uint64_t reserved =
        round_up(kListCnt * sizeof(ZoneEntryStack), kCacheLineSize) + gh_size;

    header_[shelf_num] = (void *)((char *)mapped_addr_[shelf_num] +
                                  round_up(reserved, kCacheLineSize));
    // get the zone shelf
    ret = pool_.GetShelfPath((ShelfIndex)(shelf_num + 1), path);
    if (ret != NO_ERROR) {
        // unmap the region
        ret = region_->Unmap(mapped_addr_[shelf_num], headersize);
        return HEAP_OPEN_FAILED;
    }

    // open rmb
    rmb_[shelf_num] =
        new ShelfHeap(path, ShelfId(pool_id_, (ShelfIndex)(shelf_num + 1)));
    assert(rmb_[shelf_num] != NULL);
    ret = rmb_[shelf_num]->Open(header_[shelf_num], headersize - reserved);
    if (ret != NO_ERROR) {
        // unmap the region
        ret = region_->Unmap(mapped_addr_[shelf_num], headersize);
        LOG(error) << "Zone: rmb open failed " << (uint64_t)pool_id_;
        return HEAP_OPEN_FAILED;
    }
    bitmap_start_[shelf_num] = (void *)((char *)header_[shelf_num] +
                                        rmb_[shelf_num]->get_bitmap_offset());

    // Validation to check if shelf size and shelf size in gh_ is same
    if (rmb_[shelf_num]->Size() != shelfsize) {
        LOG(error) << "Zone: Shelf size in header and actual shelf different, "
                      "shelf id: "
                   << shelf_num + 1;
        (void)region_->Unmap(mapped_addr_[shelf_num], headersize);
        (void)rmb_[shelf_num]->Close();
        return HEAP_OPEN_FAILED;
    }
    rmb_addr_[shelf_num] = rmb_[shelf_num]->GetAddr();
    rmb_size_[shelf_num] = rmb_[shelf_num]->Size();
    total_mapped_shelfs_ = shelf_num + 1;
    return NO_ERROR;
}

// The caller of this function has to ensure shelf_num is a valid shelf
ErrorCode EpochZoneHeap::CloseShelf(int shelf_num) {
    ErrorCode ret;
    ret = rmb_[shelf_num]->Close();
    if (ret != NO_ERROR) {
        return HEAP_CLOSE_FAILED;
    }
    delete rmb_[shelf_num];
    rmb_[shelf_num] = NULL;

    size_t header_size = get_header_shelf_size(shelf_num);

    // unmap the region
    ret = region_->Unmap(mapped_addr_[shelf_num], header_size);
    if (ret != NO_ERROR) {
        return HEAP_CLOSE_FAILED;
    }
    mapped_addr_[shelf_num] = NULL;
    header_[shelf_num] = NULL;
    global_list_[shelf_num] = NULL;
    rmb_size_[shelf_num] = 0;
    rmb_addr_[shelf_num] = NULL;
    return NO_ERROR;
}

// This function is invoked by processes which want to open the shelf resized
// from other processes.
ErrorCode EpochZoneHeap::OpenNewShelfs() {
    int new_total_shelfs = get_total_data_shelfs();
    ErrorCode ret;

    // Verify if we have already mapped all the shelfs
    if (new_total_shelfs == total_mapped_shelfs_)
        return NO_ERROR;

    for (int shelf_num = total_mapped_shelfs_; shelf_num < new_total_shelfs;
         shelf_num++) {

        ret = OpenShelf(shelf_num);
        if (ret != NO_ERROR) {
            LOG(error) << "ZoneHeap: OpenShelf  " << shelf_num
                       << "failed for : " << (uint64_t)pool_id_;
            return HEAP_OPEN_FAILED;
        }
    }
    return NO_ERROR;
}
//
ErrorCode EpochZoneHeap::Map(Offset offset, size_t size, void *addr_hint,
                             int prot, void **mapped_addr) {
    CHECK_IS_OPEN();
    ErrorCode ret = NO_ERROR;
    int shelf_num = get_shelfnum_from_shelfIndexoffset(offset);
    offset = get_offset_from_shelfIndexoffset(offset);

    if (rmb_[shelf_num]->IsValidOffset(offset) == true) {
        ret = rmb_[shelf_num]->Map(offset, size, addr_hint, prot, mapped_addr);
    }
    return ret;
}

ErrorCode EpochZoneHeap::Unmap(Offset offset, void *mapped_addr, size_t size) {
    CHECK_IS_OPEN();
    ErrorCode ret = NO_ERROR;
    int shelf_num = get_shelfnum_from_shelfIndexoffset(offset);
    offset = get_offset_from_shelfIndexoffset(offset);

    if (rmb_[shelf_num]->IsValidOffset(offset) == true) {
        ret = rmb_[shelf_num]->Unmap(offset, mapped_addr, size);
    }
    return ret;
}

void *EpochZoneHeap::OffsetToLocal(Offset offset) {
    TRACE();
    ASSERT_IS_OPEN();
    void *local_ptr = NULL;
    // Offset offset = global_ptr.GetOffset();
    int shelf_num = get_shelfnum_from_shelfIndexoffset(offset);
    offset = get_offset_from_shelfIndexoffset(offset);
    local_ptr = rmb_[shelf_num]->OffsetToPtr(offset);
    return local_ptr;
}

Offset EpochZoneHeap::get_offset_from_shelfIndexoffset(Offset offset) {
    return GlobalPtr(offset).GetOffset();
    // return (Offset)(((((uint64_t)1) << (GlobalPtr::kOffsetShift)) - 1) &
    // offset);
}

// The offset will have ("ShelfIndex - 1" aka shelf_num) in it.
int EpochZoneHeap::get_shelfnum_from_shelfIndexoffset(Offset offset) {
    return (int)GlobalPtr(offset).GetShelfId().GetShelfIndex();
}

int EpochZoneHeap::get_total_data_shelfs() {
    // gh_ will be NULL during destroy heap, in that case read the
    // total shelfs from pool.
    if (gh_ != NULL) {
        // Do we need to use atomic operation here ?
        // May not since only resize can change the value
        return (int)fam_atomic_u64_read(&gh_->total_shelfs);
    } else {
        // During destroy the heap is not open.
        // We need to get total shelf from pool
        ShelfIndex shelf_idx = pool_.Size();
        ErrorCode ret;
        // Find the next free shelf, if it returns
        // free shelf not found, return kMaxShelfCount.
        ret = pool_.FindNextFreeShelf(shelf_idx);
        if (ret != NO_ERROR) {
            if (ret == POOL_SHELF_NOT_FOUND)
                shelf_idx = Pool::kMaxShelfCount;
            else
                return -1; // This is not expected, return -1.
        }
        return (int)shelf_idx - 1;
    }
}

size_t EpochZoneHeap::get_header_shelf_size(int shelf_num) {
    return fam_atomic_u64_read(&gh_->sz[shelf_num].headersize);
}

//
// header size of a shelf should be read from shelf_heap.
// In addition to it we have epoch free list and global header.
//
size_t EpochZoneHeap::get_header_size_from_size(size_t size,
                                                size_t min_alloc_size,
                                                ShelfIndex shelf_idx) {
    size_t gh_size = 0;
    size_t total_size = 0;
    if (shelf_idx == 0)
        gh_size = round_up(sizeof(struct GlobalHeader), kCacheLineSize);
    total_size = ShelfHeap::get_header_size(size, min_alloc_size) +
                 round_up(kListCnt * sizeof(ZoneEntryStack), kCacheLineSize) +
                 gh_size;
    total_size = round_up(total_size, header_size_round_up());
    return total_size;
}

size_t EpochZoneHeap::get_total_header_size() {
    int total_shelfs = get_total_data_shelfs();
    size_t total_header_size = 0;
    for (int shelf_num = 0; shelf_num < total_shelfs; shelf_num++) {
        total_header_size +=
            fam_atomic_u64_read(&gh_->sz[shelf_num].headersize);
    }
    return total_header_size;
}

size_t EpochZoneHeap::get_total_size() {
    int total_shelfs = get_total_data_shelfs();
    size_t total_size = 0;
    for (int shelf_num = 0; shelf_num < total_shelfs; shelf_num++) {
        total_size += fam_atomic_u64_read(&gh_->sz[shelf_num].shelfsize);
    }
    return total_size;
}

void *EpochZoneHeap::GlobalToLocal(GlobalPtr global_ptr) {
    TRACE();
    ASSERT_IS_OPEN();
    void *local_ptr = NULL;
    Offset offset = global_ptr.GetOffset();
    ShelfId shelf_id = global_ptr.GetShelfId();
    ShelfIndex shelf_idx = shelf_id.GetShelfIndex();
    int shelf_num = shelf_idx - 1;
    local_ptr = rmb_[shelf_num]->OffsetToPtr(offset);
    return local_ptr;
}

void EpochZoneHeap::delayed_free_fn() {
    TRACE();
    ASSERT_IS_OPEN();
    Offset offset;

    for (int shelf_num = 0; shelf_num < total_mapped_shelfs_; shelf_num++) {
        if (fam_atomic_u64_read(&gh_->destroy_in_progress)) {
            is_invalid_ = true;
            for (int shelf_num = 0; shelf_num < total_mapped_shelfs_;
                 shelf_num++) {
                rmb_[shelf_num]->MarkInvalid();
            }
            Close();
            return;
        }
        uint64_t i = 0;
        EpochManager *em = EpochManager::GetInstance();
        for (; i < kFreeCnt; i++) {

            {
                EpochOp op(em);
                EpochCounter e = op.reported_epoch();
                offset = global_list_[shelf_num][(e) % kListCnt].pop(
                             bitmap_start_[shelf_num]) *
                         min_obj_size_;
            }
            if (offset == 0)
                break;
            LOG(trace) << " freeing block [" << offset << "]";
            rmb_[shelf_num]->Free(offset);
        }

        if (fam_atomic_u64_read(&gh_->destroy_in_progress)) {
            is_invalid_ = true;
            for (int shelf_num = 0; shelf_num < total_mapped_shelfs_;
                 shelf_num++) {
                rmb_[shelf_num]->MarkInvalid();
            }
            Close();
            return;
        }
        LOG(trace) << " in total " << i << " blocks have been freed";
    }
}

ErrorCode EpochZoneHeap::getStartAddress(int &numShelfs, void **&address,
                                         size_t *&Shelfsize) {
    numShelfs = total_mapped_shelfs_;
    address = rmb_addr_;
    Shelfsize = rmb_size_;
    return NO_ERROR;
}

int EpochZoneHeap::StartWorker() {
    // start the cleaner thread
    std::lock_guard<std::mutex> mutex(cleaner_mutex_);
    if (cleaner_start_ == true) {
        LOG(trace) << " cleaner thread is already started...";
        return 0;
    }
    cleaner_start_ = true;
    cleaner_stop_ = false;
    cleaner_running_ = false;
    cleaner_thread_ = std::thread(&EpochZoneHeap::BackgroundWorker, this);
    return 0;
}

int EpochZoneHeap::StopWorker() {
    // signal the cleaner to stop
    {
        std::lock_guard<std::mutex> mutex(cleaner_mutex_);
        if (cleaner_running_ == false || cleaner_start_ == false) {
            LOG(trace) << " cleaner thread is not running...";
            return 0;
        }
        cleaner_stop_ = true;
    }

    // join the cleaner thread
    if (cleaner_thread_.joinable()) {
        cleaner_thread_.join();
    }

    {
        std::lock_guard<std::mutex> mutex(cleaner_mutex_);
        cleaner_start_ = false;
        cleaner_stop_ = false;
        cleaner_running_ = false;
    }
    return 0;
}

void EpochZoneHeap::BackgroundWorker() {
    TRACE();
    ASSERT_IS_OPEN();
    Offset offset;

    while (1) {

        // check if we are shutting down...
        {
            std::lock_guard<std::mutex> mutex(cleaner_mutex_);
            if (cleaner_running_ == false) {
                cleaner_running_ = true;
                LOG(trace) << "cleaner: running...";
                running_cv_.notify_all();
            }
            if (cleaner_stop_ == true) {
                LOG(trace) << "cleaner: exiting...";
                return;
            }
        }
        LOG(trace) << "cleaner: sleep";
        usleep(kWorkerSleepMicroSeconds);
        LOG(trace) << "cleaner: wakeup";
        // do work
        for (int shelf_num = 0; shelf_num < total_mapped_shelfs_; shelf_num++) {
            if (fam_atomic_u64_read(&gh_->destroy_in_progress)) {
                is_invalid_ = true;
                for (int shelf_num = 0; shelf_num < total_mapped_shelfs_;
                     shelf_num++) {
                    rmb_[shelf_num]->MarkInvalid();
                }
                Close();
                LOG(trace) << "cleaner: exiting...";
                return;
            }
            uint64_t i = 0;
            EpochManager *em = EpochManager::GetInstance();
            if (fast_alloc_) {
                for (; i < kFreeCnt; i++) {

                    {
                        EpochOp op(em);
                        EpochCounter e = op.reported_epoch();
                        offset = global_list_[shelf_num][e % kListCnt].pop(
                                     bitmap_start_[shelf_num]) *
                                 min_obj_size_;
                    }
                    if (offset == 0)
                        break;
                    // TODO: a crash here will leak memory
                    LOG(trace) << " freeing block [" << offset << "]";
                    rmb_[shelf_num]->Free(offset);
                }
            } else {
                EpochOp op(em);
                EpochCounter e = op.reported_epoch();
                for (; i < kFreeCnt; i++) {
                    offset = global_list_[shelf_num][e % kListCnt].pop(
                                 bitmap_start_[shelf_num]) *
                             min_obj_size_;
                    if (offset == 0)
                        break;
                    // TODO: a crash here will leak memory
                    LOG(trace) << " freeing block [" << offset << "]";
                    rmb_[shelf_num]->Free(offset);
                }
            }
            if (fam_atomic_u64_read(&gh_->destroy_in_progress)) {
                is_invalid_ = true;
                for (int shelf_num = 0; shelf_num < total_mapped_shelfs_;
                     shelf_num++) {
                    rmb_[shelf_num]->MarkInvalid();
                }
                Close();
                LOG(trace) << "cleaner: exiting...";
                return;
            }
            LOG(trace) << " in total " << i << " blocks have been freed";
        }
    }
}

// TODO: Currently do the below function for only heap 0.
size_t EpochZoneHeap::MinAllocSize() { return rmb_[0]->MinAllocSize(); }

void EpochZoneHeap::Merge() {
    ASSERT_IS_OPEN();
    OpenNewShelfs();
    // TODO: Handle errors from merge
    for (int shelf_num = 0; shelf_num < total_mapped_shelfs_; shelf_num++)
        rmb_[shelf_num]->Merge();
}

void EpochZoneHeap::OfflineRecover() {
    ASSERT_IS_OPEN();
    OpenNewShelfs();
    fam_atomic_u64_write(&gh_->op_in_progress, 0);
    // TODO: Handle errors from OfflineRecover
    for (int shelf_num = 0; shelf_num < total_mapped_shelfs_; shelf_num++)
        rmb_[shelf_num]->OfflineRecover();
}

void EpochZoneHeap::OnlineRecover() {
    ASSERT_IS_OPEN();
    OpenNewShelfs();
    // TODO: Handle errors from OnlineRecover
    for (int shelf_num = 0; shelf_num < (int)total_mapped_shelfs_; shelf_num++)
        rmb_[shelf_num]->OnlineRecover();
}

void EpochZoneHeap::Stats() {
    ASSERT_IS_OPEN();
    OpenNewShelfs();
    // TODO: Handle errors from Stats
    for (int shelf_num = 0; shelf_num < (int)total_mapped_shelfs_; shelf_num++)
        rmb_[shelf_num]->Stats();
}

/* 
 * Offlinefree will free delayed free items from all the epoch queues.
 * It parses queues from all the shelfs.
 *
 * Note : This doesnt take epochOp lock, operate with care. 
 * It has to be called only when user is certain that all the items can be 
 * freed.
 */
void EpochZoneHeap::OfflineFree() {
    ASSERT_IS_OPEN();
    OpenNewShelfs();    
    for (int shelf_num = 0; shelf_num < (int)total_mapped_shelfs_; shelf_num++) {
         for(int e = 0; e < kListCnt; e++) {
             while (1) {
                   Offset offset = global_list_[shelf_num][e].pop(
                      bitmap_start_[shelf_num]) * min_obj_size_;
                   if (offset == 0)
                      break;
                   // TODO: a crash here will leak memory
                   LOG(trace) << " freeing block [" << offset << "]";
                   rmb_[shelf_num]->Free(offset);
            }
         }
    }
}

} // namespace nvmm
